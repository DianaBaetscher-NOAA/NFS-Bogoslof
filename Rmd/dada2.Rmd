---
title: "dada2"
author: "dsb"
date: "2024-04-30"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries}
library(dada2)
library(dplyr)
```

```{r}
# file location
path <- "/genetics/edna/workdir/northernfurseals/trimmed"

path
#list.files(path)
```


```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

```{r}
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])
```

Read quality... not amazing universally. Probably good to trim somewhat liberally.



```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Initial filtering truncLen failed miserably. We changed it to be super-stringent to see how that affects downstream processing.
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(110,110),
              maxN=0, maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
head(out)
```

Those filters worked decently for now - keeping a majority of the reads.


```{r}
# ensure there are no samples without reads
# as these don't work in the next steps
out %>%
  as.data.frame() %>%
  filter(reads.out <1)

# filter the matrix to retain samples with >0 reads
```

Let's move fwd with these for now... and come back if there are other issues.

### Error rates
```{r}
# fwd error rates
errF <- learnErrors(filtFs, multithread=TRUE)

# reverse error rates
errR <- learnErrors(filtRs, multithread=TRUE)

# plot the erors
p1 <- plotErrors(errF, nominalQ=TRUE)
p2 <- plotErrors(errR, nominalQ=TRUE)

p1
p2
```

### Sample inference

```{r}
# forwards
dadaFs <- dada(filtFs, err=errF, pool="pseudo", multithread=TRUE)


# reverses
dadaRs <- dada(filtRs, err=errR, pool="pseudo", multithread=TRUE)


```

Merge paired end reads
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

Make a sequence table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
Let's remove the singletons and off-target sequences
```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 168:172]
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Calculate frequency of chimeras
```{r}
sum(seqtab.nochim)/sum(seqtab2)

```

Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

## Export files for taxonomy and samples/ASVs

```{r regseqs-asv-output}
 #make fasta file with ASVs
    asv_seqs=colnames(seqtab.nochim)
    for(i in 1:length(asv_seqs))
    {
        write.table(paste(">ASV",i, sep=""),file="csv_outputs/NFS_MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
        write.table(paste(asv_seqs[i], sep=""),file="csv_outputs/NFS_MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
    }
```

That's the input for the FASTA blastn search.


Goal: change ASV headers to numbered ASVs that correspond to those output in the FASTA file.
```{r first-for-poolseqs}
# Make map between brief names and full sequences
briefToSeq <- colnames(seqtab.nochim)
names(briefToSeq) <- paste0("ASV", seq(ncol(seqtab.nochim))) # Seq1, Seq2, ...
# Make new sequence table with brief names
st.brief <- seqtab.nochim
colnames(st.brief) <- names(briefToSeq)

# export the pool seq table with brief names:
write.csv(st.brief, file="csv_outputs/NFS_MiFish_ASVtable.csv")
```

Now move on to the blastn taxonomy search.

