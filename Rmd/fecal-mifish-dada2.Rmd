---
title: "NFS_bogo_fecal_mifish_dada2"
author: "dsb"
date: "2024-09-24"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Fecal samples metabarcoded with MiFish and sequenced on an Illumina MiSeq.

Lab work by Jamie Musbach

Primers were trimmed using Cutadapt prior to processing here.


```{r load-libraries}
library(dada2)
library(dplyr)
```

```{r}
# file location
path <- "/genetics/edna/workdir/northernfurseals/bogo_fecal/trimmed/"

path
list.files(path)
```


```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

```{r}
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])
```

Looks like high quality sequencing data - which coincides with the MiSeq specs as well.


```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```



These parameters typically need to be tweaked substantially to deal with data quality (and amplicon length).
In this case, I've tested relaxing the truncLen parameter to allow for more reads to merge and now am increasing the maxEE and removing the truncLen to see if that helps retain reads.

- removing the truncLen parameter decreased the number of reads retained.
- Kim suggested using 100 for both the fwd and rev reads.
```{r filter-and-trim-PE-reads}
# modify these parameters - sometimes substantially
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(100,100),
              maxN=0, maxEE=c(2,2), truncQ=c(2,2), rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
head(out)
```


Although it looks like these filters keep a fair number of reads, the real test is with the merger step.

```{r}
# ensure there are no samples without reads
# as these don't work in the next steps
out %>%
  as.data.frame() %>%
  filter(reads.out <1)

# filter the matrix to retain samples with >0 reads
```

Let's move fwd with these for now... and come back if there are other issues.

### Error rates
```{r}
# fwd error rates
errF <- learnErrors(filtFs, multithread=TRUE)

# reverse error rates
errR <- learnErrors(filtRs, multithread=TRUE)

# plot the erors
p1 <- plotErrors(errF, nominalQ=TRUE)
p2 <- plotErrors(errR, nominalQ=TRUE)

p1
p2
```

### Sample inference

```{r sample-inference-with-dada}
# I tend to use the pseudo pool option 
# to retain rare ASVs across samples

# forwards
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)


# reverses
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

```

Merge paired end reads
```{r}
# too many reads are getting filtered out during the merge step!
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE,
                      minOverlap = 10,
                      maxMismatch = 0) # adding parameters to see if that helps...
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

```

Make a sequence table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```


```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
Let's remove the singletons and off-target sequences
```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 162:182] # expanded the range based on Kim's data analysis
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Calculate frequency of chimeras
```{r}
sum(seqtab.nochim)/sum(seqtab2)

```

Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
```


## Export files for taxonomy and samples/ASVs

```{r regseqs-asv-output}
 #make fasta file with ASVs
    asv_seqs=colnames(seqtab.nochim)
    for(i in 1:length(asv_seqs))
    {
        write.table(paste(">ASV",i, sep=""),file="csv_outputs/NFSfecal_MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
        write.table(paste(asv_seqs[i], sep=""),file="csv_outputs/NFSfecal_MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
    }
```

That's the input for the FASTA blastn search.


Goal: change ASV headers to numbered ASVs that correspond to those output in the FASTA file.
```{r first-for-poolseqs}
# Make map between brief names and full sequences
briefToSeq <- colnames(seqtab.nochim)
names(briefToSeq) <- paste0("ASV", seq(ncol(seqtab.nochim))) # Seq1, Seq2, ...
# Make new sequence table with brief names
st.brief <- seqtab.nochim
colnames(st.brief) <- names(briefToSeq)

# export the pool seq table with brief names:
write.csv(st.brief, file="csv_outputs/NFSfecal_MiFish_ASVtable.csv")
```

Now move on to the blastn taxonomy search.
