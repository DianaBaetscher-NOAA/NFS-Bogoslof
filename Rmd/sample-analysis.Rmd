---
title: "sample-analysis"
author: "diana baetscher"
date: "2024-04-30"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load libraries 
```{r, message=FALSE}
library(tidyverse)
library(dplyr)
#library(rstan)
library(broom)
library(vegan)
library(reshape)

```


load ASV table and metadata
```{r}
# read in asv table
asv_table <- read_csv("../data/NFS_MiFish_ASVtable.csv") %>%
  dplyr::rename(sample = `...1`) # need to call the dplyr function


# transpose and remove entries for zero reads
asv_tbl_long <- asv_table %>%
  pivot_longer(2:length(asv_table), values_to = "reads", names_to = "ASV") %>%
  filter(reads > 0) 

asv_tbl_long <- asv_tbl_long %>%
  separate(., sample, into = c("extraction_ID", "replicate", "X"), remove = F)

```


```{r}
# add some metadata to evaluate samples
meta <- read_csv("../data/eDNA_extraction_database - Sheet1.csv") %>%
  filter(str_detect(project, "fur seal"))

meta
```

```{r}
# add taxonomy
tax <- read_csv("csv_outputs/nfs_mifish_taxonomy.csv") %>%
  select(-`...1`)

# this is the uncollapsed taxonomy
# so I'll need to further filter/etc.
# to get a single assignment
# and then collapse that down to just a single taxon per ASV
collapsed_taxonomy <- tax %>%
  select(qseqid, taxon, taxonomic_level) %>%
  unique() %>%
  filter(!taxon %in% c("Canis lupus"),
         !taxonomic_level %in% c("class", "order"))

collapsed_taxonomy
```


```{r}
# add metadata to sample/ASV table
asv_w_meta <- asv_tbl_long %>%
  left_join(., meta, by = "extraction_ID") #%>%
  # left_join(., tax_top_ranked, by = c("ASV" = "qseqid")) %>%
  # filter(!is.na(taxon))

# quick look at controls
asv_w_meta %>%
  filter(!is.na(X)) %>%
  ggplot(aes(x = sample, y = reads, fill = ASV)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90)
  )
  
```

```{r}
# field blanks?
asv_w_meta %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  filter(sample_type == "field_blank",
         !is.na(taxon)) %>%
  ggplot(aes(x = sample, y = reads, fill = taxon)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90)
  )

```

That particular ASV didn't pass the blast filtering step... I should look at Kim's decontamination protocol again.


```{r}
# samples
asv_w_meta %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  filter(!is.na(taxon)) %>%
  filter(sample_type == "sample",
        # reads > 10,
         location2 == "north") %>%
  ggplot(aes(x = sample, y = reads, fill = taxon)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(location2), space = "free", scales = "free") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90)
  )

```
```{r make-df-for-comparison}
plate30_norm <- asv_w_meta %>%
  filter(extraction_plate == "30_2023") %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
   filter(!is.na(taxon) &
           sample != "Undetermined")

plate30_norm %>%
  write_csv(file = "csv_outputs/plate30_norm_df.csv")
```


```{r check-plate30-against-the-reruns}
asv_w_meta %>%
  filter(extraction_plate == "30_2023") %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
   filter(!is.na(taxon) &
           sample != "Undetermined") %>% 
  group_by(location2, location3, taxon) %>%
  mutate(reads_by_site = sum(reads)) %>%
  select(reads_by_site, location2, location3, taxon) %>%
  unique() %>%
  mutate(location3 = ifelse(is.na(location3), "fieldBlank", location3)) %>%
  unite(location2, location3, col = "site") %>%
  ggplot(aes(x = site, y = reads_by_site, fill = taxon)) +
  geom_bar(stat = "identity") +
  #facet_wrap(~site, scales = "free") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.9, vjust = 0.5)
  ) +
  labs(
    title = "Reads summed across field/lab replicates by site"
  )
  
#ggsave("pdf_outputs/plate30_normalized_quickplot.pdf", height = 11, width = 10)
ggsave("pdf_outputs/plate30_normalized_site.pdf", height = 5, width = 8)


```

Still need to think about the evenness of coverage across samples/reps.



```{r}
# what about just taking the top taxonomic assignment?
tax %>%
  filter(taxonomic_level %in% c("family", "genus")) %>%
  group_by(qseqid) %>%
  slice_max(., order_by = seq_percID, n = 2, with_ties = T)


```



```{r}
tax %>%
  filter(class == "Mammalia" & species != "Canis lupus") %>%
  arrange(species) %>%
  left_join(., asv_w_meta, by = c("qseqid" = "ASV")) %>%
  select(species) %>%
  unique()
```


## Testing Kim's decontamination procedure


load ASV table and metadata
load ASV table and metadata
```{r}
# read in asv table
asv_table <- read_csv("../data/NFS_MiFish_ASVtable.csv") %>%
  dplyr::rename(sample = `...1`) 

# transpose and remove entries for zero reads
asv_tbl_long <- asv_table %>%
  pivot_longer(2:length(asv_table), values_to = "reads", names_to = "ASV") %>%
  filter(reads > 0) 

asv_tbl_long <- asv_tbl_long %>%
  separate(., sample, into = c("extraction_ID", "replicate", "X"), remove = F)

  
```

add column to the ASV table that labels the sample type

```{r}
# add some metadata to evaluate samples
asv_table_with_sample_type <- meta %>%
  dplyr::select(extraction_ID, sample_type, collection_year, project, extraction_date, extraction_plate) %>%
  full_join(asv_tbl_long, by = c("extraction_ID")) %>%
  #filter(str_detect(replicate, "plate")) %>% # fixing the different number of columns for the controls
  mutate(replicate = ifelse(str_detect(replicate, "plate"), X, replicate)) %>%
  select(-X) 

```

### positive controls 

let's start by visualizing the reads in the positive control samples 
```{r}
asv_table_with_sample_type %>%
  filter(str_detect(sample, "PC")) %>%
  ggplot(aes(x=sample, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads in positive controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
```

top asvs in positive controls
```{r}
asvs_PC <- asv_table_with_sample_type %>%
  filter(str_detect(sample, "PC")) %>%
  group_by(ASV) %>%
  summarise(cum_reads = sum(reads)) %>%
  arrange(desc(cum_reads))
  
```

### pcr blanks 

let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  filter(str_detect(sample, "NC")) %>%
  ggplot(aes(x=sample, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr negatives") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

### field blanks 

let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=sample, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(project~collection_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr negatives") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
asvs_FC <- asv_table_with_sample_type %>%
  filter(sample_type == "field_blank") %>%
  group_by(ASV, collection_year, project) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))
```



## 1. Estimate index hopping  
subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(extraction_ID == "PC") %>%
  group_by(sample) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop))

prop_asvs_in_positives
```

Considerations for tag-jumping...
I will also need to remove all reads belonging to ASV8 (the PC) at some point



## 2. Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. 

let's start by taking a look at what reads remain in these controls 
```{r}
asv_table_with_sample_type <- asv_table_with_sample_type %>%
  mutate(sample_type = ifelse(str_detect(sample, "NC"), "PCR_blank", sample_type)) %>%
  mutate(sample_type = ifelse(str_detect(sample, "PC"), "positive_control", sample_type))

asv_table_with_sample_type %>%
  filter(sample_type != "sample") %>%
  ggplot(aes(x=sample, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(~sample_type, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

number of reads
```{r}
reads_per_type_ASV <- asv_table_with_sample_type %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(is.na(sample))

head(not_in_samples)
```
There are 17 ASVs exclusively in the field blanks - seems fair game to remove all of those.
```{r}
# check taxonomy?
not_in_samples %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid"))
```
All of these would be reasonable things to find...
Take an look at whether there are a fair number of ASVs for atka mackerel, e.g.
```{r}
collapsed_taxonomy %>%
  filter(taxon == "Pleurogrammus monopterygius")

```
Wow, 32 ASVs for atka mackerel. Seems wild.



what ASVs have reads in samples, but more reads in the controls?  
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>% 
  filter(PCR_blank > sample)
head(more_in_pcr_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive_control > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```

There's our positive control (ASV8) showing up in the field blank, PC, and sample.

And three ASVs (102, 35, 90) showing up in greater numbers in the field blanks rather than the samples.

```{r peek-at-taxonomy-prior-to-filtering}
# take a quick look at the identity of those ASVs before removing them
collapsed_taxonomy %>%
  filter(qseqid %in% c("ASV102", "ASV35", "ASV90", "ASV8"))

```

okay, want to remove asv8 since it's the pc. 
what about asvs that are greater in field blanks?   
ASV102 - Oncorhynchus
ASV35 - Sebastidae
ASV90 - Gadidae

relatively low numbers of reads of these asvs in samples, so the conservative approach is to remove them 


remove these from the data frame 
```{r}
asv_table_filter1 <- asv_table_with_sample_type %>%
  filter(!ASV %in% not_in_samples$ASV) %>%
  filter(!ASV %in% more_in_pc_blanks$ASV) %>%
  filter(!ASV %in% more_in_fb_blanks$ASV)
```


how much does this change things in the controls?
```{r plot-controls-without-filtered-ASVs}
asv_table_filter1 %>%
  filter(sample_type != "sample") %>%
  ggplot(aes(x=sample, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(~sample_type, scales = "free") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )

```

```{r}
asv_table_filter1 %>%
  filter(sample_type %in% c("sample", "field_blank") &
           reads >10000) %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  filter(ASV == "ASV3")

```
No taxonomic info that goes along with ASV3. But it is found in very large read numbers in both the field blanks and a couple of samples.

## Is it appropriate to subtract reads from samples?
This step is set up to subtract the maximum number of reads from ASVs found in the extraction and pcr negative controls from all samples.

calculate the maximum number of reads in an ASV to still show up in an extraction or PCR negative control 
```{r}
reads_to_remove_per_ASV <- asv_table_filter1 %>%
  filter(sample_type == "extraction_blank"| sample_type == "PCR_blank") %>%
  group_by(ASV) %>%
  summarize(max_reads = max(reads)) %>%
  arrange(desc(max_reads))

reads_to_remove_per_ASV %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid"))

## does it make sense to remove these reads from samples?

asv_table_filter1 %>% 
  filter(sample_type == "sample") %>%
  inner_join(., reads_to_remove_per_ASV, by = "ASV") %>%
  filter(max_reads > reads)
# these are the entries that would be lost with this approach.


# for asv2 (herring), subtracting reads would eliminate the asv from SBS 2022 samples, and just reduce reads in NBS 2021 and NBS 2022 samples
# for asv9 (pink salmon), subtracting reads would eliminate the asv from nearly all samples 

reads_to_remove_per_sample <- asv_table_filter1 %>%
  left_join(reads_to_remove_per_ASV, by = "ASV") %>%
  mutate(read_minus_contamination = reads - max_reads) %>%
  mutate(read_minus_contamination = if_else(read_minus_contamination < 0, 0, read_minus_contamination))
```
I don't love removing these because they are likely true observations.


Moving on without altering sample read numbers based on controls for now...

## 3. Discard PCR replicates with low numbers of reads 

calculate reads per sample
```{r}
all_reads <- asv_table_filter1 %>%
  group_by(sample) %>%
  summarize(ReadsPerSample = sum(reads))
```

visualize 
```{r}
all_reads$x_reordered <- reorder(all_reads$sample, -all_reads$ReadsPerSample)

all_reads %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity")
```
```{r}
# which samples are retained?
all_reads %>%
  filter(ReadsPerSample > 1000) %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity") +
  theme(
    axis.text.x = element_text(angle = 90)
  )

```
The number of reads for a given sample are kind of all over the place.


fit a normal distribution
```{r}
fit <- MASS::fitdistr(all_reads$ReadsPerSample, "normal")

all_reads %>%  
  mutate(prob = pnorm(all_reads$ReadsPerSample, fit$estimate[[1]], fit$estimate[[2]])) -> all_reads
```

identify and remove the outliers
```{r}
low_dist_probability_cutoff <- 0.05
minimum_read_cutoff <- 1000

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff | ReadsPerSample < minimum_read_cutoff)
  #filter(ReadsPerSample < minimum_read_cutoff)
  #filter(prob < low_dist_probability_cutoff)

outlierIDs <- outliers$sample
```

which samples are removed because of the 1000 reads threshold??
```{r}
replicates_removed <- asv_table_filter1 %>%
  filter(sample %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")

```

number of pcr replicates removed
```{r}
nrow(replicates_removed)
```

## Taking a pause in Kim's decontamination protocol to explore the idiosycracies of this dataset.

Why are there so many replicates with very low read depth?
Take a look at by sample variation.
```{r exploring-variation-across-sample-replicates}
asv_table_with_sample_type %>%
  #group_by(extraction_ID) %>%
  ggplot(aes(x = extraction_ID, y = reads, color = ASV)) +
  facet_grid(rows = vars(extraction_plate), cols = vars(replicate), scales = "free") +
  geom_point() +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none"
  )


```
Seems like more of the samples from replicate 3 got very little data?
Do we think that the extracts are very low concentration, or that there's more variation on the lab side of things?

```{r}
# what about some averages by replicate plate?
asv_table_with_sample_type %>%
  group_by(extraction_plate, replicate) %>%
  summarise(mean_reads = mean(reads)) %>%
  filter(!is.na(extraction_plate))

```
There was one full plate: plate 29, and one partial plate: plate 30. 
Very weird that the replicates on plate 30 got very different numbers of reads. 
Meanwhile, the third replicate for plate 29 got so few reads.



But even the first and second reps have ~600 reads different on average.

Reads is one metric, but what about composition?

```{r ASV-composition-across-replicates?}
asv_table_with_sample_type %>%
  filter(!is.na(extraction_plate)) %>%
  ggplot(aes(x = extraction_ID, y = reads, fill = ASV)) +
  geom_bar(stat = "identity") +
  facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90)
  ) 

ggsave("pdf_outputs/sampleQC_by_replicate_reads_w_ASV.png", width = 12, height = 5)

```

What about reads by plate, since all three replicates for some samples were on the same plate?



What if I look at it by proportions rather than just reads? Does that make it appear better or worse?

```{r}
asv_table_with_sample_type %>%
  group_by(extraction_ID, replicate) %>%
  mutate(total_reads = sum(reads), 
         prop_reads = reads/total_reads) %>%
  ggplot(aes(x = extraction_ID, y = prop_reads, fill = ASV)) +
  geom_bar(stat = "identity") +
  facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90)
  ) 
  

ggsave("pdf_outputs/sampleQC_by_replicate_w_ASV.png", width = 12, height = 5)
```

Let's make the same plot, but excluding ASVs without taxonomy??

```{r ASVs-with-taxonomy-by-replicate}
asv_table_with_sample_type %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  group_by(extraction_ID, replicate) %>%
  mutate(total_reads = sum(reads), 
         prop_reads = reads/total_reads) %>%
  ggplot(aes(x = extraction_ID, y = prop_reads, fill = taxon)) +
  geom_bar(stat = "identity") +
  facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, size = 8)
  ) 

ggsave("pdf_outputs/sampleQC_by_replicate_w_taxonomy.png", width = 12, height = 8)
```


A lot of the reads are going to ASVs that don't have any taxonomic assignment.


```{r ASVs-with-taxonomy-by-replicate-filtered}
extra_asvs <- read_csv("csv_outputs/human_dog_asvs.csv") %>%
  dplyr::select(qseqid, sscinames)

asv_table_with_sample_type %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  left_join(., extra_asvs, by = c("ASV" = "qseqid")) %>%
  mutate(taxon = ifelse(is.na(taxon), sscinames, taxon)) %>%
  group_by(extraction_ID, replicate) %>%
  mutate(total_reads = sum(reads), 
         prop_reads = reads/total_reads) %>%
    filter(total_reads > 1000) %>%
  ggplot(aes(x = extraction_ID, y = prop_reads, fill = taxon)) +
  geom_bar(stat = "identity") +
  facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, size = 8)
  ) 

ggsave("pdf_outputs/sampleQC_by_replicate_w_taxonomy_1000read_min.png", width = 12, height = 8)

```
Yes - a bunch of the NA ASVs are Homo Sapiens. To me that suggests low initial concentrations.

How many samples have <5000 reads? That was a threshold that ended up looking weird in the original 2021 NBS dataset.

```{r}
asv_table_with_sample_type %>% 
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  left_join(., extra_asvs, by = c("ASV" = "qseqid")) %>%
  mutate(taxon = ifelse(is.na(taxon), sscinames, taxon)) %>%
  group_by(extraction_ID, replicate) %>%
  mutate(total_reads = sum(reads), 
         prop_reads = reads/total_reads) %>% filter(extraction_ID == "e02704") %>%
   # filter(total_reads > 10000) %>% 
  ggplot(aes(x = extraction_ID, y = prop_reads, fill = taxon)) +
  geom_bar(stat = "identity") +
  facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, size = 8)
  ) 

ggsave("pdf_outputs/sampleQC_by_replicate_w_taxonomy_5000read_min.png", width = 12, height = 8)

```

There's a lot more concordance with >5,000 reads.


Can I add some metadata to get a better sense for variation within/across field samples?

```{r}
asv_table_with_sample_type %>%
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  left_join(., extra_asvs, by = c("ASV" = "qseqid")) %>%
  mutate(taxon = ifelse(is.na(taxon), sscinames, taxon)) %>%
  group_by(extraction_ID, replicate) %>%
  mutate(total_reads = sum(reads), 
         prop_reads = reads/total_reads) %>%
  left_join(., meta_select) %>%
  group_by(collection_day, location2)
  
```


Read in lat/lon information and convert to decimal degrees
```{r}
loc_info <- readxl::read_xlsx("../data/Bogoslof_eDNA_2023.xlsx")

loc_dms <- loc_info %>%
  separate(longitude, into = c("lon_deg", "lon_min", "lon_sec", "y", "lon_hem")) %>%
    unite(lon_sec, 3:4, sep = ".") %>%
    separate(latitude, into = c("lat_deg", "lat_min", "lat_sec", "x", "lat_hem")) %>%
    unite(lat_sec, 7:8, sep = ".") 

# convert character to numeric
loc_dms[1:3] <- sapply(loc_dms[1:3], as.numeric)
loc_dms[5:7] <- sapply(loc_dms[5:7], as.numeric)
sapply(loc_dms, class)

loc_dms
```


```{r fct-to-convert-dms-to-decimal}
dms_to_decimal <- function(degrees, minutes, seconds, hemisphere) {
  decimal <- degrees + minutes / 60 + seconds / 3600
  if (hemisphere %in% c('S', 'W')) {
    decimal <- -decimal
  }
  return(decimal)
}
```


```{r convert-dms-to-decimal-degrees}
loc_dms$lat <- with(loc_dms, mapply(dms_to_decimal, lat_deg, lat_min, lat_sec, lat_hem))
loc_dms$lon <- with(loc_dms, mapply(dms_to_decimal, lon_deg, lon_min, lon_sec, lon_hem))

locs_decimal <- loc_dms %>%
  select(lat, lon, Year, Month, Day, Transect, StationKM, DepthM, SampleNumber, SampleType, Sampler1, Sampler2, Notes)

locs_decimal$SampleNumber <- as.character(locs_decimal$SampleNumber)

# add more metadata to that
full_meta <- locs_decimal %>%
  left_join(., meta, by = c("SampleNumber" = "alternative_ID"))

full_meta %>%
  select(extraction_ID, everything()) %>%
  write_csv("../data/Bogoslof_full_metadata.csv")

add_meta <- full_meta %>%
  select(extraction_ID, SampleNumber, lat, lon, Transect, StationKM, SampleType)
```


Look at groups of samples together
```{r combine-sample-table-with-metadata}
# add metadata
asv_table_with_sample_type %>%
  left_join(., add_meta, by = "extraction_ID") %>%
  filter(str_detect(SampleNumber, "10")) %>% # just to make it more tractable.
  ggplot(aes(x = extraction_ID, y = reads, fill = ASV)) +
  geom_bar(stat = "identity") +
  facet_wrap(~SampleNumber, scales = "free") +
  #facet_grid(rows = vars(replicate), cols = vars(extraction_plate), scales = "free", space = "free") +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90)
  ) 


```

## Checking a few more things before re-doing some PCRs

```{r}
asv_w_meta %>%
  filter(!is.na(extraction_plate)) %>% # ignore the controls for now
  #mutate(extraction_plate = ifelse(str_detect(replicate, "plate"), replicate, extraction_plate)) %>%
  group_by(sample) %>%
  mutate(total_reads = sum(reads)) %>%
  select(sample, total_reads, extraction_plate, replicate) %>%
  unique() %>%
  group_by(extraction_plate, replicate) %>%
  summarise(mean_reads = mean(total_reads))

```

```{r look-more-closely-at-variation-across-plate30}
asv_w_meta %>%
  filter(extraction_plate == "30_2023" &
           extraction_ID == "e02710") %>%
  group_by(sample) %>%
  filter(sample_type != "field_blank") %>%
  mutate(total_reads = sum(reads)) %>%
  select(extraction_ID, sample, total_reads, extraction_plate, replicate, sample_type) %>%
  unique() %>%
  ggplot(aes(x = replicate, y = total_reads)) +
  geom_bar(stat = "identity") +
  #facet_grid(rows = vars(sample_type), cols = vars(extraction_ID), space = "free")
  facet_wrap(~extraction_ID)
  
```

Add some metadata about sample type?

```{r}
asv_w_meta %>%
  filter(extraction_plate == "29_2023") %>%
  group_by(sample) %>%
  filter(sample_type != "field_blank") %>%
  mutate(total_reads = sum(reads)) %>%
  select(extraction_ID, sample, total_reads, extraction_plate, replicate, sample_type) %>%
  unique() %>%
  ggplot(aes(x = replicate, y = total_reads)) +
  geom_bar(stat = "identity") +
  #facet_grid(rows = vars(sample_type), cols = vars(extraction_ID), space = "free")
  facet_wrap(~extraction_ID)
  
ggsave("pdf_outputs/plate29_asv_reads_by_rep.png", width = 10, height = 10)
```


## Summarize across all samples from one bottle (site)

```{r}
asv_w_meta %>%
  filter(!is.na(location2)) %>%
  group_by(location2, location3) %>%
  summarise(total_reads_per_Niskin = sum(reads)) %>%
  ggplot(aes(x = location3, y = total_reads_per_Niskin)) +
  facet_grid(rows = vars(location2)) +
  geom_point()

```

```{r}
asv_w_meta %>%
  filter(!is.na(location2)) %>%
  group_by(location2, location3, ASV) %>%
  summarise(total_reads_per_ASV = sum(reads)) %>%
  ggplot(aes(x = location3, y = total_reads_per_ASV, fill = ASV)) +
  facet_grid(rows = vars(location2)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  theme(
    legend.position = "none"
  )

```
That's all ASV, not just those with taxonomic assignments.

What does it look like if we keep just the ASVs with assignments?
```{r}
asv_w_meta %>%
  # add taxonomy
  left_join(., collapsed_taxonomy, by = c("ASV" = "qseqid")) %>%
  filter(!is.na(location2) &
           !is.na(taxon)) %>%
  group_by(location2, location3, taxon) %>%
  summarise(total_reads_per_taxon = sum(reads)) %>%
  ggplot(aes(x = location3, y = total_reads_per_taxon, fill = taxon)) +
  facet_grid(rows = vars(location2)) +
  geom_bar(stat = "identity") +
  theme_bw()


```
Ok, so the question is whether this plot ends up looking similar to the new version (for the sample from plate 30).

```{r}
asv_w_meta %>%
  select(extraction_plate, extraction_ID) %>%
  unique() %>%
  group_by(extraction_plate) %>%
  tally()
```


